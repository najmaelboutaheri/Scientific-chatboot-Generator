{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgWC6diH3q7"
      },
      "source": [
        "**import libraries**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVyOqyU-HOwb"
      },
      "source": [
        "# **Task 1 objective**:\n",
        "\n",
        "our goal is to implement Retrieval-Augmented Generation (RAG) to enhance the model’s ability to generate accurate and contextually relevant responses by retrieving information from a pre-defined dataset or knowledge base.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MM29pb2PHBsr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1e4c7c-6583-4bce-b07a-5130f75825bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m645.1/647.5 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# install necessary libraries\n",
        "!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6s4OPOtCJfFi",
        "outputId": "be6abc2b-b7e7-412b-9863-af4f2839314e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.26.4)\n",
            "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 17.0.0\n",
            "    Uninstalling pyarrow-17.0.0:\n",
            "      Successfully uninstalled pyarrow-17.0.0\n",
            "Successfully installed pyarrow-14.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow==14.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlRFxXvsIHwn"
      },
      "outputs": [],
      "source": [
        "from tqdm.notebook import tqdm\n",
        "import pandas as pd\n",
        "from typing import Optional, List, Tuple\n",
        "from datasets import load_dataset\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Raw knowledge Base\n",
        "from langchain.schema import Document as LangchainDocument\n",
        "# Chunking phase :\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "# Indexation phase :\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores.utils import DistanceStrategy\n",
        "\n",
        "# Visualization :\n",
        "import pacmap\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "\n",
        "# Generation phase :\n",
        "from transformers import pipeline\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "\n",
        "# Reranking :\n",
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "pd.set_option(\"display.max_colwidth\", None)  # This will be helpful when visualizing retriever outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAvxAaC5lG8I"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nnr1g55gMEyV"
      },
      "source": [
        "**load dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G6zfSYU5MLW4"
      },
      "outputs": [],
      "source": [
        "ds.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39zMVH4IMPxX"
      },
      "outputs": [],
      "source": [
        "ds.features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtFHhVa8uXUc"
      },
      "outputs": [],
      "source": [
        "# We take a sample\n",
        "\n",
        "ds_ = ds.shuffle(seed=42).select(range(20000))\n",
        "\n",
        "ds_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BiSfXSjNOMp"
      },
      "source": [
        "**A base containing documents with their richeful metadata**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_OujjZnMrSu"
      },
      "outputs": [],
      "source": [
        "RAW_KNOWLEDGE_BASE = [\n",
        "    LangchainDocument(page_content=row[\"abstract\"], metadata={\"title\": row[\"title\"], \"authors\": row[\"authors\"], \"submitter\": row[\"submitter\"], \"categories\" : row[\"categories\"],\"journal reference\" :row[\"journal-ref\"]}) for row in tqdm(ds_)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ckx9rk_MNTYs"
      },
      "outputs": [],
      "source": [
        "RAW_KNOWLEDGE_BASE[3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtn6SmdSOfJk"
      },
      "source": [
        "**Embedding the docs and chunking phase :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9OlzCEDOvwn"
      },
      "outputs": [],
      "source": [
        "Markdown_seprators = [\n",
        "    \"\\n#{1,6} \",\n",
        "    \"```\\n\",\n",
        "    \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
        "    \"\\n---+\\n\",\n",
        "    \"\\n___+\\n\",\n",
        "    \"\\n\\n\",\n",
        "    \"\\n\",\n",
        "    \" \",\n",
        "    \"\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUb6ZOYjOP6q"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"\n",
        "\n",
        "def split_documents(\n",
        "    chunk_size: int,\n",
        "    knowledge_base: List[LangchainDocument],\n",
        "    tokenizer_name: Optional[str] = EMBEDDING_MODEL_NAME,\n",
        ") -> List[LangchainDocument]:\n",
        "    \"\"\"\n",
        "    Split documents into chunks of maximum size `chunk_size` tokens and return a list of documents.\n",
        "    \"\"\"\n",
        "    text_splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "        AutoTokenizer.from_pretrained(tokenizer_name),\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=int(chunk_size / 10),\n",
        "        add_start_index=True,\n",
        "        strip_whitespace=True,\n",
        "        separators=Markdown_seprators,\n",
        "    )\n",
        "\n",
        "    docs_processed = []\n",
        "    for doc in tqdm(knowledge_base):\n",
        "        docs_processed += text_splitter.split_documents([doc])\n",
        "\n",
        "    # Remove duplicates\n",
        "    unique_texts = {}\n",
        "    docs_processed_unique = []\n",
        "    for doc in tqdm(docs_processed):\n",
        "        if doc.page_content not in unique_texts:\n",
        "            unique_texts[doc.page_content] = True\n",
        "            docs_processed_unique.append(doc)\n",
        "\n",
        "    return docs_processed_unique\n",
        "\n",
        "# Splitting the documents\n",
        "\n",
        "\n",
        "docs_processed = split_documents(\n",
        "    512,  # We choose a chunk size adapted to our model\n",
        "    RAW_KNOWLEDGE_BASE,\n",
        "    tokenizer_name=EMBEDDING_MODEL_NAME,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbsZYqzaA4Kz"
      },
      "source": [
        "Lets save the processed docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jo_fPpc_A3ut"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "# Save the processed documents to a file using pickle\n",
        "with open('docs_processed.pkl', 'wb') as f:\n",
        "    pickle.dump(docs_processed, f)\n",
        "\n",
        "print(\"docs_processed has been saved as docs_processed.pkl\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btkbkmNlO2Hr"
      },
      "source": [
        "**Indexation : loading the embedded chunks into the Faiss Vector DB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3OsFKHy-vJE"
      },
      "outputs": [],
      "source": [
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=EMBEDDING_MODEL_NAME,\n",
        "    multi_process=True,\n",
        "    model_kwargs={\"device\": \"cuda\"},\n",
        "    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n",
        ")\n",
        "\n",
        "KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
        "    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the FAISS index to disk\n",
        "KNOWLEDGE_VECTOR_DATABASE.save_local('/content/drive/MyDrive/3DpresentationF/codes/index.faiss')\n"
      ],
      "metadata": {
        "id": "LW6KGGTlkJeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2VVn9zGO8lP"
      },
      "outputs": [],
      "source": [
        "# Embed a user query in the same space\n",
        "user_query = \"Which authors did discuss interesting theories in mathematics?\"\n",
        "query_vector = embedding_model.embed_query(user_query)\n",
        "print(query_vector)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZrPPVmiPBAC"
      },
      "outputs": [],
      "source": [
        "# Retrieving docs related to the user's query\n",
        "print(f\"\\nStarting retrieval for {user_query=}...\")\n",
        "retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "print(\"\\n==================================Top document==================================\")\n",
        "print(retrieved_docs[0].page_content)\n",
        "print(\"==================================Metadata==================================\")\n",
        "print(retrieved_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hsFd_qVRaui"
      },
      "source": [
        "**Build Reader (Generation) Model :**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aZDhzxKRbyG"
      },
      "outputs": [],
      "source": [
        "READER_MODEL_NAME = \"HuggingFaceH4/zephyr-7b-alpha\"\n",
        "\n",
        "# Quantization :\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    READER_MODEL_NAME,\n",
        "    quantization_config=bnb_config\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(READER_MODEL_NAME)\n",
        "\n",
        "READER_LLM = pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task=\"text-generation\",\n",
        "    do_sample=True,\n",
        "    temperature=0.2,\n",
        "    repetition_penalty=1.1,\n",
        "    return_full_text=False,\n",
        "    max_new_tokens=500\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfe6-6thBn9v"
      },
      "source": [
        "Let's save the **Reader LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HxxupdZVBnri"
      },
      "outputs": [],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained('quantized_model/')\n",
        "tokenizer.save_pretrained('quantized_model/')\n",
        "\n",
        "print(\"Model and tokenizer saved to 'quantized_model/' directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEnSi7DISh6r"
      },
      "source": [
        "**RAG_PROMPT_TEMPLATE creation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPq4mfWdSVMB"
      },
      "outputs": [],
      "source": [
        "prompt_in_chat_format = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\"\"Using the information contained in the context,\n",
        "give a comprehensive detailled answer to the question.\n",
        "Respond only to the question asked, response should be concise and relevant to the question.\n",
        "Provide the name of the authors, do not provide the number of the document.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\"\"\",\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\"\"Context:\n",
        "{context}\n",
        "---\n",
        "Now here is the question you need to answer.\n",
        "\n",
        "Question: {question}\"\"\",\n",
        "    },\n",
        "]\n",
        "RAG_PROMPT_TEMPLATE = tokenizer.apply_chat_template(\n",
        "    prompt_in_chat_format, tokenize=False, add_generation_prompt=True\n",
        ")\n",
        "print(RAG_PROMPT_TEMPLATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8XJbE-7Srht"
      },
      "source": [
        "**Constructing the context to pass to the generator LLM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLfofZoRSl-g"
      },
      "outputs": [],
      "source": [
        "retrieved_docs_text = [doc.page_content for doc in retrieved_docs]  # We only need the text of the documents\n",
        "context = \"\\nExtracted documents:\\n\"\n",
        "context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n",
        "\n",
        "# First test :\n",
        "final_prompt = RAG_PROMPT_TEMPLATE.format(question=\"Show me papers related to quantum computing with a focus on error correction!\", context=context)\n",
        "\n",
        "# Redact an answer\n",
        "answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhBDn08HTJ0m"
      },
      "source": [
        "**Combine all process in one function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOpU8OPpSwmH"
      },
      "outputs": [],
      "source": [
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index: FAISS,\n",
        "    reranker: Optional[RAGPretrainedModel] = None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[LangchainDocument]]:\n",
        "    # Gather documents with retriever\n",
        "    print(\"=> Retrieving documents...\")\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]  # Keep only the text\n",
        "\n",
        "    # Optionally rerank results\n",
        "    if reranker:\n",
        "        print(\"=> Reranking documents...\")\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    # Build the final prompt\n",
        "    context = \"\\nExtracted documents:\\n\"\n",
        "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs)])\n",
        "\n",
        "    final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
        "\n",
        "    # Redact an answer\n",
        "    print(\"=> Generating answer...\")\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "\n",
        "    return answer, relevant_docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vs-LMwDw9viw"
      },
      "outputs": [],
      "source": [
        "# Rerabking the retried docs : improve the process of retrieval\n",
        "RERANKER = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJzbhMkHSxhe"
      },
      "outputs": [],
      "source": [
        "question = \"Show me papers related to quantum computing with a focus on error correction!\"\n",
        "\n",
        "answer, relevant_docs = answer_with_rag(question, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XWMx3cU5UHGC"
      },
      "outputs": [],
      "source": [
        "print(\"==================================Answer==================================\")\n",
        "print(f\"{answer}\")\n",
        "print(\"==================================Source docs==================================\")\n",
        "for i, doc in enumerate(relevant_docs):\n",
        "    print(f\"Document {i}------------------------------------------------------------\")\n",
        "    print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqE-JR4wUlYN"
      },
      "source": [
        "**chatbot_response function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TubrZ-TCUhYU"
      },
      "outputs": [],
      "source": [
        "def chatbot_response(user_query: str) :\n",
        "  retrieved_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "  retrieved_docs_text = [doc.page_content for doc in retrieved_docs]  # We only need the text of the documents\n",
        "  context = \"\\nExtracted documents:\\n\"\n",
        "  context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(retrieved_docs_text)])\n",
        "  final_prompt = RAG_PROMPT_TEMPLATE.format(question=user_query, context=context)\n",
        "\n",
        "  # Redact an answer\n",
        "  answer = READER_LLM(final_prompt)[0][\"generated_text\"]\n",
        "  return f\"Echo: {answer}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOix4SpGBCn8"
      },
      "outputs": [],
      "source": [
        "chatbot_response(user_query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM9AFcitBuVp"
      },
      "outputs": [],
      "source": [
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSRGEcghBJ0q"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# Define the chatbot interface\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_response,       # The function to call for each user input\n",
        "    inputs=\"text\",             # The input type\n",
        "    outputs=\"text\",            # The output type\n",
        "    title=\"Simple Echo Chatbot\" # Title of the interface\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pVllRmm_hWE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmsI2sbj_jKx"
      },
      "source": [
        "Lets test if **app.py** works well with model resources"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}