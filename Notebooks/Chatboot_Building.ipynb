{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d8789408",
      "metadata": {
        "papermill": {
          "duration": 0.00516,
          "end_time": "2024-10-30T12:00:34.043621",
          "exception": false,
          "start_time": "2024-10-30T12:00:34.038461",
          "status": "completed"
        },
        "tags": [],
        "id": "d8789408"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In scientific research, accessing relevant information quickly and accurately can significantly improve the experimentation and reporting process. Researchers often need to retrieve documents, datasets, and past experimental results to make informed decisions and ensure their work is built upon verified data. This project presents a Retrieval-Augmented Generation (RAG) chatbot specifically designed to streamline the retrieval of documents within a research environment, enabling users to seamlessly access, update, and utilize information throughout their scientific workflows.\n",
        "\n",
        "## Objectives of the RAG Chatbot Project\n",
        "\n",
        "- **Efficient Document Access:** Enable researchers to access key documents quickly, whether for background information, experimental procedures, or data analysis.\n",
        "- **Enhanced Research Documentation:** Support the consistent logging of experiments to promote rigor and reproducibility.\n",
        "- **Accelerate Reporting:** Automate portions of the report generation process, aligning with scientific publishing standards.\n",
        "- **Improve Experiment Quality:** Through real-time anomaly detection and feedback, researchers gain insights into their data as it is generated."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1db71692",
      "metadata": {
        "papermill": {
          "duration": 0.004207,
          "end_time": "2024-10-30T12:00:34.052517",
          "exception": false,
          "start_time": "2024-10-30T12:00:34.048310",
          "status": "completed"
        },
        "tags": [],
        "id": "1db71692"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "543d74c7",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-10-30T12:00:34.063032Z",
          "iopub.status.busy": "2024-10-30T12:00:34.062666Z",
          "iopub.status.idle": "2024-10-30T12:01:20.388450Z",
          "shell.execute_reply": "2024-10-30T12:01:20.387502Z"
        },
        "id": "543d74c7",
        "outputId": "95af1ac6-c1f1-4737-f9d6-124d3293c01e",
        "papermill": {
          "duration": 46.333982,
          "end_time": "2024-10-30T12:01:20.390833",
          "exception": false,
          "start_time": "2024-10-30T12:00:34.056851",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/647.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m647.5/647.5 kB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m171.5/171.5 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m66.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.8/176.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for annoy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Necessary dependancies\n",
        "!pip install -q torch transformers transformers accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl pacmap datasets langchain-community ragatouille streamlit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8613baf",
      "metadata": {
        "papermill": {
          "duration": 0.004283,
          "end_time": "2024-10-30T12:01:20.399889",
          "exception": false,
          "start_time": "2024-10-30T12:01:20.395606",
          "status": "completed"
        },
        "tags": [],
        "id": "d8613baf"
      },
      "source": [
        "### Build the chatboot app file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21dd5398",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-30T12:01:20.410814Z",
          "iopub.status.busy": "2024-10-30T12:01:20.410500Z",
          "iopub.status.idle": "2024-10-30T12:01:20.422010Z",
          "shell.execute_reply": "2024-10-30T12:01:20.421170Z"
        },
        "papermill": {
          "duration": 0.019803,
          "end_time": "2024-10-30T12:01:20.424105",
          "exception": false,
          "start_time": "2024-10-30T12:01:20.404302",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21dd5398",
        "outputId": "47f6764b-f149-4f2c-baeb-520520649c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app_chatbot.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app_chatbot.py\n",
        "# Streamlit App\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "import pandas as pd\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from ragatouille import RAGPretrainedModel\n",
        "\n",
        "# Configure pandas\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "# Caching the RAG model\n",
        "@st.cache_resource\n",
        "def load_rag_model():\n",
        "    return RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
        "\n",
        "# Caching the embedding model\n",
        "@st.cache_resource\n",
        "def load_embedding_model():\n",
        "    return HuggingFaceEmbeddings(\n",
        "        model_name=\"thenlper/gte-small\",\n",
        "        multi_process=True,\n",
        "        model_kwargs={\"device\": \"cuda\"},\n",
        "        encode_kwargs={\"normalize_embeddings\": True},\n",
        "    )\n",
        "\n",
        "# Function to load FAISS database\n",
        "@st.cache_resource\n",
        "def load_faiss_database(_embedding_model):\n",
        "    return FAISS.load_local(\n",
        "        \"/content/drive/MyDrive/3DpresentationF/codes\",\n",
        "        _embedding_model,\n",
        "        allow_dangerous_deserialization=True\n",
        "    )\n",
        "\n",
        "# Load all models at the beginning\n",
        "embedding_model = load_embedding_model()\n",
        "KNOWLEDGE_VECTOR_DATABASE = load_faiss_database(embedding_model)\n",
        "RERANKER = load_rag_model()\n",
        "\n",
        "# Caching the LLM\n",
        "@st.cache_resource\n",
        "def load_llm():\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        \"HuggingFaceH4/zephyr-7b-alpha\",\n",
        "        quantization_config=bnb_config\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
        "    return pipeline(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        task=\"text-generation\",\n",
        "        do_sample=True,\n",
        "        temperature=0.2,\n",
        "        repetition_penalty=1.1,\n",
        "        return_full_text=False,\n",
        "        max_new_tokens=500\n",
        "    )\n",
        "\n",
        "READER_LLM = load_llm()\n",
        "\n",
        "# Function to answer queries with RAG\n",
        "def answer_with_rag(\n",
        "    question: str,\n",
        "    llm: pipeline,\n",
        "    knowledge_index,\n",
        "    reranker=None,\n",
        "    num_retrieved_docs: int = 30,\n",
        "    num_docs_final: int = 5,\n",
        ") -> Tuple[str, List[str]]:\n",
        "    relevant_docs = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
        "    relevant_docs = [doc.page_content for doc in relevant_docs]\n",
        "\n",
        "    if reranker:\n",
        "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
        "        relevant_docs = [doc[\"content\"] for doc in relevant_docs]\n",
        "\n",
        "    relevant_docs = relevant_docs[:num_docs_final]\n",
        "\n",
        "    context = \"\\nExtracted documents:\\n\" + \"\".join([f\"Document {i}:::\\n{doc}\" for i, doc in enumerate(relevant_docs)])\n",
        "    final_prompt = f\"Using the information contained in the context,\\ngive a detailed answer to the question.\\nRespond only to the question asked.\\nIf the answer cannot be deduced from the context, do not give an answer.\\nContext:\\n{context}\\n\\n---\\n\\nQuestion: {question}\"\n",
        "\n",
        "    answer = llm(final_prompt)[0][\"generated_text\"]\n",
        "    return answer, relevant_docs\n",
        "\n",
        "# User Profiles Configuration\n",
        "USER_PROFILES = {\n",
        "    \"student\": {\n",
        "    \"additional_context\": \"\"\"\n",
        "    Assume the student has a basic understanding of the subject but is unfamiliar with more advanced concepts.\n",
        "    Start by introducing the topic with simple terms and gradually move towards the more complex ideas.\n",
        "    Use analogies and examples to explain difficult concepts, and break down any technical terms or jargon.\n",
        "    At the end of your explanation, provide a real-world example or application to solidify the student’s understanding.\n",
        "    If relevant, include a step-by-step guide on how to approach solving a related problem.\n",
        "    \"\"\",\n",
        "    \"response_style\": \"clear, detailed, and engaging with simple language\",\n",
        "    \"max_tokens\": 700\n",
        "},\n",
        "    \"researcher\": {\n",
        "        \"additional_context\": \"Provide concise, precise, and technical responses, focusing on the relevant research findings.\",\n",
        "        \"response_style\": \"concise and technical\",\n",
        "        \"max_tokens\": 300\n",
        "    },\n",
        "    \"domain_expert\": {\n",
        "        \"additional_context\": \"Give a highly technical explanation assuming the user has deep expertise in the subject matter.\",\n",
        "        \"response_style\": \"highly technical\",\n",
        "        \"max_tokens\": 1000\n",
        "    }\n",
        "}\n",
        "\n",
        "# Function to get prompt template based on user profile\n",
        "def get_prompt_template(user_profile: str, context: str, question: str) -> str:\n",
        "    if user_profile not in USER_PROFILES:\n",
        "        raise ValueError(f\"User profile '{user_profile}' not found. Available profiles: {list(USER_PROFILES.keys())}\")\n",
        "\n",
        "    profile_data = USER_PROFILES[user_profile]\n",
        "\n",
        "    base_prompt = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": f\"\"\"Using the information contained in the context,\n",
        "give a {profile_data['response_style']} answer to the question.\n",
        "Respond only to the question asked. Provide the names of the authors, do not give document numbers.\n",
        "If the answer cannot be deduced from the context, do not give an answer.\n",
        "{profile_data['additional_context']}\"\"\"\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"\"\"Context:\n",
        "{context}\n",
        "\n",
        "---\n",
        "\n",
        "Question: {question}\"\"\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    return base_prompt\n",
        "\n",
        "# Streamlit interface\n",
        "st.title(\"RAG-based Scientific Chatbot\")\n",
        "\n",
        "# Partie de l'interface pour afficher l'historique des questions dans la barre latérale\n",
        "if 'qa_history' not in st.session_state:\n",
        "    st.session_state.qa_history = []\n",
        "# Initialize sidebar visibility state\n",
        "if 'sidebar_open' not in st.session_state:\n",
        "    st.session_state.sidebar_open = True  # Sidebar is open by default\n",
        "\n",
        "# Function to toggle sidebar visibility\n",
        "def toggle_sidebar():\n",
        "    st.session_state.sidebar_open = not st.session_state.sidebar_open\n",
        "\n",
        "with st.sidebar:\n",
        "    st.header(\"Historique des questions\")\n",
        "    if st.session_state.qa_history:\n",
        "        for i, (q, a) in enumerate(st.session_state.qa_history):\n",
        "            if st.button(f\"Question {i+1}: {q[:30]}...\"):  # Limitez l'affichage de la question\n",
        "                st.session_state['selected_question'] = q\n",
        "                st.session_state['selected_answer'] = a\n",
        "\n",
        "# Input: sélection du profil utilisateur\n",
        "user_profile = st.selectbox(\"Select your profile:\", options=list(USER_PROFILES.keys()))\n",
        "\n",
        "# Input: question de l'utilisateur\n",
        "user_query = st.chat_input(\"Enter your query:\")\n",
        "\n",
        "# Afficher la question et la réponse sélectionnées\n",
        "if 'selected_question' in st.session_state:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(st.session_state['selected_question'])\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.write(st.session_state['selected_answer'])\n",
        "\n",
        "if user_query:\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(user_query)\n",
        "\n",
        "    with st.spinner('Retrieving relevant documents...'):\n",
        "        relevant_docs = KNOWLEDGE_VECTOR_DATABASE.similarity_search(query=user_query, k=5)\n",
        "\n",
        "    context = \"\\nExtracted documents:\\n\" + \"\".join(\n",
        "        [f\"**Title:** {doc.metadata['title']}\\n**Authors:** {doc.metadata['authors']}\\n**Submitter:** {doc.metadata['submitter']}\\n**Categories:** {doc.metadata['categories']}\\n**Journal Reference:** {doc.metadata['journal reference']}\\n\\n**Content:**\\n{doc.page_content}\\n\\n\" for doc in relevant_docs])\n",
        "\n",
        "    prompt = get_prompt_template(user_profile, context, user_query)\n",
        "\n",
        "    with st.spinner('Generating an answer...'):\n",
        "        answer, _ = answer_with_rag(user_query, READER_LLM, KNOWLEDGE_VECTOR_DATABASE)\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.write(answer)\n",
        "\n",
        "    # Enregistrer dans l'historique\n",
        "    st.session_state.qa_history.append((user_query, answer))\n",
        "\n",
        "    # Afficher les documents récupérés\n",
        "    with st.expander(\"Retrieved Documents\"):\n",
        "        for i, doc in enumerate(relevant_docs):\n",
        "            st.markdown(f\"#### Document {i + 1}\")\n",
        "            st.markdown(f\"**Title:** {doc.metadata['title']}\")\n",
        "            st.markdown(f\"**Authors:** {doc.metadata['authors']}\")\n",
        "            st.markdown(f\"**Submitter:** {doc.metadata['submitter']}\")\n",
        "            st.markdown(f\"**Categories:** {doc.metadata['categories']}\")\n",
        "            st.markdown(f\"**Journal Reference:** {doc.metadata['journal reference']}\")\n",
        "            st.markdown(f\"**Content:**\\n{doc.page_content}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6w1H_Cybo95",
        "outputId": "4e67e88c-f35c-4a32-db0f-8ba66908ca91"
      },
      "id": "l6w1H_Cybo95",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f5e0208",
      "metadata": {
        "papermill": {
          "duration": 0.004278,
          "end_time": "2024-10-30T12:01:20.432983",
          "exception": false,
          "start_time": "2024-10-30T12:01:20.428705",
          "status": "completed"
        },
        "tags": [],
        "id": "9f5e0208"
      },
      "source": [
        "### Testing the app using Streamlit and Ngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6c7dca0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-30T12:01:20.443517Z",
          "iopub.status.busy": "2024-10-30T12:01:20.442954Z",
          "iopub.status.idle": "2024-10-30T12:01:32.279626Z",
          "shell.execute_reply": "2024-10-30T12:01:32.278693Z"
        },
        "id": "b6c7dca0",
        "outputId": "9da32c96-8f38-43f8-8fdf-33546a305181",
        "papermill": {
          "duration": 11.844575,
          "end_time": "2024-10-30T12:01:32.282067",
          "exception": false,
          "start_time": "2024-10-30T12:01:20.437492",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.40.0-py2.py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog<6,>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n",
            "Downloading streamlit-1.40.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: watchdog, pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.40.0 watchdog-5.0.3\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a60526",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-30T12:01:32.295718Z",
          "iopub.status.busy": "2024-10-30T12:01:32.295364Z",
          "iopub.status.idle": "2024-10-30T12:01:44.316193Z",
          "shell.execute_reply": "2024-10-30T12:01:44.314994Z"
        },
        "id": "43a60526",
        "outputId": "3f6b13d5-27bc-4eb8-c643-6bdcb4060602",
        "papermill": {
          "duration": 12.029679,
          "end_time": "2024-10-30T12:01:44.318504",
          "exception": false,
          "start_time": "2024-10-30T12:01:32.288825",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n",
            "Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ddfec14",
      "metadata": {
        "papermill": {
          "duration": 0.005494,
          "end_time": "2024-10-30T12:01:44.330052",
          "exception": false,
          "start_time": "2024-10-30T12:01:44.324558",
          "status": "completed"
        },
        "tags": [],
        "id": "3ddfec14"
      },
      "source": [
        "Dowloand Ngrok packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f47a1592",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-30T12:01:44.343420Z",
          "iopub.status.busy": "2024-10-30T12:01:44.343052Z",
          "iopub.status.idle": "2024-10-30T12:01:50.163829Z",
          "shell.execute_reply": "2024-10-30T12:01:50.162619Z"
        },
        "id": "f47a1592",
        "outputId": "d1ef429d-73c0-45af-85f4-888a9d9867c2",
        "papermill": {
          "duration": 5.83046,
          "end_time": "2024-10-30T12:01:50.166519",
          "exception": false,
          "start_time": "2024-10-30T12:01:44.336059",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok.zip\n",
            "  inflating: ngrok                   \n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "# Remove existing ngrok if any\n",
        "#!rm -f ngrok\n",
        "\n",
        "# Then run the download and unzip commands\n",
        "!wget -q -O ngrok.zip https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip -o ngrok.zip\n",
        "\n",
        "# Set the ngrok authtoken\n",
        "\n",
        "# Add your ngrok auth token\n",
        "!ngrok config add-authtoken 2mLv0yhsmDVpduryAaw83Tv1SdT_4oBGFiWVzEw497U4CWwSu\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7f3e09c",
      "metadata": {
        "papermill": {
          "duration": 0.007043,
          "end_time": "2024-10-30T12:01:50.180993",
          "exception": false,
          "start_time": "2024-10-30T12:01:50.173950",
          "status": "completed"
        },
        "tags": [],
        "id": "c7f3e09c"
      },
      "source": [
        "Setup the App environment"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ngrok.kill()"
      ],
      "metadata": {
        "id": "nEFrGf9fxP01"
      },
      "id": "nEFrGf9fxP01",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f471d994",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-30T12:01:50.197885Z",
          "iopub.status.busy": "2024-10-30T12:01:50.196904Z",
          "iopub.status.idle": "2024-10-30T12:01:50.736528Z",
          "shell.execute_reply": "2024-10-30T12:01:50.735612Z"
        },
        "id": "f471d994",
        "papermill": {
          "duration": 0.550078,
          "end_time": "2024-10-30T12:01:50.738697",
          "exception": false,
          "start_time": "2024-10-30T12:01:50.188619",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Run Streamlit app\n",
        "def run_streamlit():\n",
        "    process = subprocess.Popen(['streamlit', 'run', 'app_chatbot.py'])\n",
        "    return process\n",
        "\n",
        "# Establish ngrok tunnel\n",
        "def start_ngrok():\n",
        "    public_url = ngrok.connect(8501)\n",
        "    print(f\"Streamlit app is live at: {public_url}\")\n",
        "    return public_url\n",
        "\n",
        "# Start Streamlit and ngrok\n",
        "process = run_streamlit()\n",
        "url = start_ngrok()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mlpicgHTXAzW"
      },
      "id": "mlpicgHTXAzW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xUCFv-9FsnkO"
      },
      "id": "xUCFv-9FsnkO"
    },
    {
      "cell_type": "markdown",
      "id": "ec8913a9",
      "metadata": {
        "papermill": {
          "duration": 0.008093,
          "end_time": "2024-10-30T12:01:50.753964",
          "exception": false,
          "start_time": "2024-10-30T12:01:50.745871",
          "status": "completed"
        },
        "tags": [],
        "id": "ec8913a9"
      },
      "source": [
        "Explore Ngrok tunnels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d0464bd",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-10-30T12:01:50.770732Z",
          "iopub.status.busy": "2024-10-30T12:01:50.770254Z",
          "iopub.status.idle": "2024-10-30T12:01:51.944124Z",
          "shell.execute_reply": "2024-10-30T12:01:51.943066Z"
        },
        "papermill": {
          "duration": 1.185332,
          "end_time": "2024-10-30T12:01:51.946718",
          "exception": false,
          "start_time": "2024-10-30T12:01:50.761386",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3d0464bd",
        "outputId": "707ac008-5f97-45b6-da41-5bda7bfc5c50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok - tunnel local ports to public URLs and inspect traffic\n",
            "\n",
            "USAGE:\n",
            "  ngrok [command] [flags]\n",
            "\n",
            "AUTHOR:\n",
            "  ngrok - <support@ngrok.com>\n",
            "\n",
            "COMMANDS: \n",
            "  config          update or migrate ngrok's configuration file\n",
            "  http            start an HTTP tunnel\n",
            "  tcp             start a TCP tunnel\n",
            "  tunnel          start a tunnel for use with a tunnel-group backend\n",
            "\n",
            "EXAMPLES: \n",
            "  ngrok http 80                                                 # secure public URL for port 80 web server\n",
            "  ngrok http --url baz.ngrok.dev 8080                           # port 8080 available at baz.ngrok.dev\n",
            "  ngrok tcp 22                                                  # tunnel arbitrary TCP traffic to port 22\n",
            "  ngrok http 80 --oauth=google --oauth-allow-email=foo@foo.com  # secure your app with oauth\n",
            "\n",
            "Paid Features: \n",
            "  ngrok http 80 --url mydomain.com                              # run ngrok with your own custom domain\n",
            "  ngrok http 80 --cidr-allow 2600:8c00::a03c:91ee:fe69:9695/32  # run ngrok with IP policy restrictions\n",
            "  Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Upgrade your account at https://dashboard.ngrok.com/billing/subscription to access paid features\n",
            "\n",
            "Flags:\n",
            "  -h, --help      help for ngrok\n",
            "\n",
            "Use \"ngrok [command] --help\" for more information about a command.\n"
          ]
        }
      ],
      "source": [
        "!ngrok tunnels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c044b803",
      "metadata": {
        "papermill": {
          "duration": 0.007066,
          "end_time": "2024-10-30T12:01:51.961374",
          "exception": false,
          "start_time": "2024-10-30T12:01:51.954308",
          "status": "completed"
        },
        "tags": [],
        "id": "c044b803"
      },
      "source": [
        "# Conclusion\n",
        "The RAG-based document retrieval chatbot developed in this project demonstrates a valuable tool for enhancing scientific research workflows. By combining retrieval-augmented generation capabilities with real-time interaction, the chatbot efficiently meets the need for quick access to research documents ans experiment tracking. This integration of AI-driven document retrieval allows researchers to focus on the substance of their work, reducing the time spent on administrative tasks and document searching."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 5740326,
          "sourceId": 9445215,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5764633,
          "sourceId": 9477857,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5864693,
          "sourceId": 9611240,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30776,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 83.173542,
      "end_time": "2024-10-30T12:01:54.587838",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2024-10-30T12:00:31.414296",
      "version": "2.6.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}